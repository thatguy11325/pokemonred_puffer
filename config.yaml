wandb:
  entity: thatguy11325
  project: pokemon
  group: ~

debug:
  env:
    headless: False
    stream_wrapper: False
    init_state: "victory_road_5"
    state_dir: pyboy_states
    max_steps: 21000
    log_frequency: 1
    disable_ai_actions: True
    use_global_map: False
    reduce_res: True
    animate_scripts: True
    save_state: False
    auto_next_elevator_floor: True
    auto_solve_strength_puzzles: True
  train:
    device: cpu
    compile: False
    compile_mode: default
    num_envs: 1
    envs_per_worker: 1
    num_workers: 1
    env_batch_size: 128
    zero_copy: False
    batch_size: 1024
    minibatch_size: 128
    batch_rows: 4
    bptt_horizon: 2
    total_timesteps: 1_000_000
    save_checkpoint: True
    checkpoint_interval: 4
    save_overlay: True
    overlay_interval: 1
    verbose: False
    env_pool: False
    load_optimizer_state: False
    async_wrapper: False
    sqlite_wrapper: True
    archive_states: False

env:
  headless: True
  save_final_state: True
  print_rewards: True
  video_dir: video
  state_dir: pyboy_states
  init_state: Bulbasaur
  action_freq: 24
  max_steps: 20480
  save_video: False
  fast_video: False
  frame_stacks: 1
  perfect_ivs: True
  reduce_res: True
  two_bit: True
  log_frequency: 2000
  auto_flash: True
  disable_wild_encounters:
    - INDIGO_PLATEAU
  disable_ai_actions: False
  auto_teach_cut: True
  auto_use_cut: True
  auto_use_strength: True
  auto_use_surf: True
  auto_teach_surf: True
  auto_teach_strength: True
  auto_solve_strength_puzzles: True
  auto_remove_all_nonuseful_items: True
  auto_pokeflute: True
  auto_next_elevator_floor: True
  skip_safari_zone: False
  infinite_safari_steps: False
  insert_saffron_guard_drinks: True
  infinite_money: True
  use_global_map: False
  save_state: True
  animate_scripts: False
  exploration_inc: 1.0
  exploration_max: 1.0
  max_steps_scaling: 0 # 0.2 # every 10 events or items gained, multiply max_steps by 2
  map_id_scalefactor: 5.0 # multiply map ids whose events have not been completed by 5

train:
  seed: 1
  torch_deterministic: True
  device: cuda
  compile: True
  compile_mode: "reduce-overhead"
  float32_matmul_precision: "high"
  total_timesteps: 250_000_000 # 1_000_000_000 # 100_000_000_000 for full games
  batch_size: 65536
  minibatch_size: 2048
  anneal_lr: False
  num_minibatches: 4
  update_epochs: 3
  norm_adv: True
  ent_coef: 0.010005
  gae_lambda: 0.9501
  gamma: 0.9980002
  clip_coef: 0.1
  clip_vloss: True
  learning_rate: 2.0e-4
  max_grad_norm: 0.5
  target_kl: ~
  vf_clip_coef: 0.1
  vf_coef: 0.5
  batch_rows: 128
  bptt_horizon: 16

  num_envs: 288
  num_workers: 24
  env_batch_size: 36
  env_pool: True
  zero_copy: False

  verbose: False
  data_dir: runs
  save_checkpoint: False
  checkpoint_interval: 200
  save_overlay: True
  overlay_interval: 100
  cpu_offload: False
  pool_kernel: [0]
  load_optimizer_state: False
  use_rnn: True
  async_wrapper: False
  sqlite_wrapper: True
  archive_states: True
  swarm: True
  early_stop:
    # event name: minutes. If we dont satisfy each condition
    # we early stop
    # The defaults have a margin of error
    EVENT_BEAT_BROCK: 30
    EVENT_BEAT_MISTY: 90
    EVENT_GOT_HM01: 180
    EVENT_BEAT_ROUTE_9_TRAINER_0: 300
  one_epoch:
    # - "EVENT_BEAT_CHAMPION_RIVAL"
    - HM_03
    - HM_04

wrappers:
  empty:
    - episode_stats.EpisodeStatsWrapper: {}

  baseline:
    - stream_wrapper.StreamWrapper:
        user: thatguy
    - exploration.DecayWrapper:
        step_forgetting_factor:
          npc: 0.995
          coords: 0.9995
          map_ids: 0.995
          explore: 0.9995
          start_menu: 0.998
          pokemon_menu: 0.998
          stats_menu: 0.998
          bag_menu: 0.998
          action_bag_menu: 0.998
        forgetting_frequency: 10
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0

  finite_coords:
    - stream_wrapper.StreamWrapper:
        user: thatguy
    - exploration.MaxLengthWrapper:
        capacity: 1750
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0

  stream_only:
    - stream_wrapper.StreamWrapper:
        user: thatguy
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 1

  fixed_reset_value:
    - stream_wrapper.StreamWrapper:
        user: thatguy
    - exploration.OnResetLowerToFixedValueWrapper:
        fixed_value:
          coords: 0.33
          map_ids: 0.33
          npc: 0.33
          cut: 0.33
          explore: 0.33
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 25
        jitter: 0
    - episode_stats.EpisodeStatsWrapper: {}

rewards:
  baseline.BaselineRewardEnv:
    reward:
  baseline.TeachCutReplicationEnv:
    reward:
      event: 1.0
      bill_saved: 5.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      moves_obtained: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 1.0
      cut_tiles: 1.0
      start_menu: 0.01
      pokemon_menu: 0.1
      stats_menu: 0.1
      bag_menu: 0.1

  baseline.TeachCutReplicationEnvFork:
    reward:
      event: 1.0
      bill_saved: 5.0
      moves_obtained: 4.0
      hm_count: 10.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 1.0
      cut_tiles: 1.0
      start_menu: 0.01
      pokemon_menu: 0.1
      stats_menu: 0.1
      bag_menu: 0.1
      taught_cut: 10.0
      explore_npcs: 0.02
      explore_hidden_objs: 0.02

  baseline.CutWithObjectRewardsEnv:
    reward:
      event: 1.0
      bill_saved: 5.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      moves_obtained: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.00
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.1
      rocket_hideout_found: 5.0
      explore_hidden_objs: 0.02
      seen_action_bag_menu: 0.1

  baseline.CutWithObjectRewardRequiredEventsEnv:
    reward:
      event: 1.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      moves_obtained: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 5.0
      exploration: 0.02
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.0
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.0
      explore_hidden_objs: 0.02
      seen_action_bag_menu: 0.0
      required_event: 5.0
      required_item: 5.0
      useful_item: 1.0
      pokecenter_heal: 1.0

  baseline.ObjectRewardRequiredEventsEnvTilesetExploration:
    reward:
      event: 1.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      moves_obtained: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 5.0
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.0
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.0
      explore_hidden_objs: 0.01
      explore_signs: 0.015
      seen_action_bag_menu: 0.0
      required_event: 5.0
      required_item: 5.0
      useful_item: 1.0
      pokecenter_heal: 0.2
      exploration: 0.02
      exploration_gym: 0.025
      exploration_facility: 0.11
      exploration_plateau: 0.025
      exploration_lobby: 0.035 # for game corner
      a_press: 0.0 # 0.00001
      explore_warps: 0.05
      use_surf: 0.05

  baseline.ObjectRewardRequiredEventsMapIds:
    reward:
      a_press: 0.0 # 0.00001
      badges: 3.0
      bag_menu: 0.0
      caught_pokemon: 2.5
      cut_coords: 0.0
      cut_tiles: 0.0
      event: .75
      exploration: 0.019
      explore_hidden_objs: 0.00009999
      explore_signs: 0.015
      explore_warps: 0.01006
      hm_count: 7.5
      level: 1.05
      moves_obtained: 4.0
      pokecenter_heal: 0.47
      pokemon_menu: 0.0
      required_event: 7.0
      required_item: 3.0
      seen_action_bag_menu: 0.0
      seen_pokemon: 2.5
      start_menu: 0.0
      stats_menu: 0.0
      use_surf: 0.4
      useful_item: 0.825
      safari_zone: 0.825

policies:
  multi_convolutional.MultiConvolutionalPolicy:
    policy:
      hidden_size: 512

    rnn:
      # Assumed to be in the same module as the policy
      name: MultiConvolutionalRNN
      args:
        input_size: 512
        hidden_size: 512
        num_layers: 1
